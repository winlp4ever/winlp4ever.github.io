<!DOCTYPE html>
<html>
    <head>
        <!-- basic set-ups -->
        <meta charset="utf8">
        <meta name='author' content="me">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <link rel="stylesheet" href="https://cdn.rawgit.com/konpa/devicon/df6431e323547add1b4cf45992913f15286456d3/devicon.min.css">
        <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css" integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">
        
        <!-- prism.css to enable code snippets with syntax highlighting on website -->
        <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.15.6/build/styles/vs.min.css">
        
        <!-- css style files (local  and fontawesome and devicon -->
        <link rel='stylesheet' href='css/style.css'>
        <link rel='stylesheet' href='css/app.css'>
        <link rel='stylesheet' href='css/navi.css'>
        <link rel='stylesheet' href='css/outline.css'>

        <!-- vega-lite to enable data visualization on website -->
        <script src="https://cdn.jsdelivr.net/npm/vega@5.3.5/build/vega.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/vega-lite@3.2.1/build/vega-lite.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/vega-embed@4.0.0/build/vega-embed.js"></script>

        <style media="screen">
            /* Add space between Vega-Embed links  */
            .vega-actions a {
                margin-right: 5px;
            }
        </style>
    </head>
    <body>
        <!-- links bar (where to share contacts) -->
        <button id='menuclick'><i class="fa fa-hashtag"></i></button>
        <div id='links'>
            <span id='ruler'></span>
            <span><a href='https://github.com/winlp4ever' target='_blank'><i class="fab fa-github"></i></a></span>
            <span><a href='https://www.facebook.com/letringuyenquang' target='_blank'><i class="fab fa-facebook"></i></a></span>
            <span><a href='https://www.linkedin.com/in/ha-quang-le/' target='_blank'><i class="fab fa-linkedin"></i></a></span>
            <div id='annotate'></div>
        </div>
        <!-- main menu - navigator -->
        <div id='navi'>
            
            
            <span id='posts'>Posts</span>
            <div class='drleft'><i class="fa fa-angle-down fa-fw"></i></div>
            <span id='coding'>Coding</span>
            <span id='dl'>DLearning</span>

            <div class='slsh'></div>
            <span id='home'>Home</span>
            <span><a href='//winlp4ever.github.io/aboutme/.' target='_blank'>My CV</a></span>
        </div>

        <div id='outline'>
            <span><i class="fa fa-hashtag"></i>Content</span>
        </div>

        <!-- mainbody, where to write posts -->
        <div id='mainbody'>
            <!-- a post, div class post -->
            <div class='post home coding' id='gmm'>
                <h1>GMM clustering visualization</h1>
                <p>
                    When it comes to clustering, <strong>Gaussian Mixture model</strong> (<b>gmm</b>) is often amongst our first choices as method. Let's find out how to visualize it in a 
                    best way:
                </p>
                <p>First, we build up an artificial data, in real situations, of course it is provided:</p>
                <!-- code snippets always enclosed in pre code flags -->
                <pre><code class="python">
                        n_samples = 1000
                        mus = [(1., 1.), (5., 2.), (3., 4.)]
                        covs = [(0.1, 0.5), (0.5, 0.6), (0.2, 0.4)]
                        alphas = [0.2, 0.5, 0.3]
                        gauss = np.random.choice(3, p=alphas, size=n_samples)
                        gauss = np.eye(3)[gaussians].astype('int')
                        
                        U = np.stack([np.random.multivariate_normal(mus[i], np.diag(covs[i]), size=n_samples) 
                                             for i in range(3)], axis=1)
                        X_train = np.zeros(shape=(n_samples, 2))
                        X_train[:, 0] = np.einsum('ij, ij->i', gauss, U[:,:, 0])
                        X_train[:, 1] = np.einsum('ij, ij->i', gauss, U[:,:, 1])
                        print(X_train.shape)
                </code></pre>
                <p>Now let define a GMM model, remember to import <code>GaussianMixture</code> model from <code>sklearn</code> already:</p>

                <pre><code class='python'>
                        gmm = GaussianMixture(n_components=3, init_params='kmeans')
                        Y = gmm.fit_predict(X_train) # model is train and output the final predictions on data
                </code></pre>

                <p>With the help of <code>contour</code> functions in <code>matplotlib</code>, we will be able to produce something pretty. The idea is 
                quite simple, we construct a grid, compute all grid nodes' values with our model, then draw contours.</p>
                <pre><code class='python'>
                        gridx, gridy = np.linspace(0, 8, num=100), np.linspace(-1, 7, num=100)# on range of points
                        x, y = np.meshgrid(gridx, gridy)
                        xy = np.array([x.ravel(), y.ravel()]).T
                        z = -gmm.score_samples(xy)
                        z = z.reshape(x.shape)
                </code></pre>
                <p>The final plotting code:</p>
                <pre><code class='python'>
                        plt.figure(figsize=(15, 10))

                        mapcolor = 'Spectral'
                        contours = plt.contour(x, y, z, norm=LogNorm(vmin=1.0, vmax=1000.0),
                                         levels=np.logspace(0, 3, 20), cmap=mapcolor, alpha=1)
                        plt.clabel(contours, inline=True, fontsize=12)
                        plt.contourf(x, y, z, norm=LogNorm(vmin=1.0, vmax=1000.0),
                                         levels=np.logspace(0, 3, 20), cmap=mapcolor, alpha=0.3)
                        plt.imshow(z, extent=[0, 8, -1, 7], origin='lower',
                                   cmap=mapcolor, alpha=0.3)
                        plt.colorbar()
                        plt.scatter(X_train[:100, 0], X_train[:100, 1], c=Y[:100], s=200, 
                                    cmap=mapcolor, alpha=0.5, linewidths=2, edgecolors='grey')
                        
                        font = {'family': 'Helvetica',
                                'color':  'black',
                                'weight': 'bold',
                                'size': 14,
                                }
                        plt.xlabel('xlabel', fontdict=font)
                        plt.ylabel('ylabel', fontdict=font)
                        plt.title('title', fontdict=font)
                        plt.show()
                </code></pre>

                <p>Here the result:</p>
                <img src='img/gmm.png' width='600'/>
            </div>
        
            <div class='post home coding' id='tokenizer'>
                <h1>Use <code>CountVectorizer</code> as a Tokenizer</h1>

                <p>People often use <code>nltk</code>'s tokenizer for text tokenization. Though it doesn't perform poorly, as a research tool it's evidently not so 
                    well optimized, and sometimes requires a lot of engineering efforts to work perfectly. There are tons of better alternatives, though, and unknown to
                many, <code>CountVectorizer</code> of <code>sklearn</code> can be used as a very good <code>tokenizer</code> due to some reasons:</p>

                <ul>
                    <li>As an industrial product, it is very well optimized, so we don't need to be very meticulous to make it work.</li>
                    <li>All useful hyperparameters are there so that your program can be done in one-pass: As a matter of fact, we don't only tokenize text, we also
                        get rid of words overwhelmingly present, or ones that rarely appear. For some tasks like topic-modeling, we even need to erase out all stop words.
                    And that's a lot of preprocessing to do. With <code>CountVectorizer</code>, we can just simply pass all demands as params to the function, and that's all 
                    </li>
                    <li>When it comes to Machine Learning, <code>sklearn</code> seems inevitable. It's a must-learn toolbox. So try to keep as much as possible works
                    inside it may be a good idea.</li>
                </ul>

                <p>So now, how do?</p>

                <p>A wanderer over the sklearn's docs may find it an over-kill, but the idea behind the model itself is quite simple. In fact, a 
                    <code>CountVectorizer</code>
                is simply composed of a <i>tokenizer</i>, who tokenizes and normalizes all tokens, then an <i>analyzer</i> to filter them by conditions.</p>

                <p>Though the model transforms texts to one-hot encodings, once fitted, its vocabulary is all what we want. With some twists, it can easily handle 
                    <strong>text-to-bow</strong>
                and <strong>text-to-boi</strong> and vice-versa:</p>
                <pre><code class='python'>
                        class Vocab(CountVectorizer):
                        def __init__(self, texts, **kwargs):
                            super(Vocab, self).__init__(**kwargs)
                            self.fit(texts)
                    
                        def __len__(self):
                            return len(self.vocabulary_)
                    
                        def _to_indices(self, sent):
                            analyzer = self.build_analyzer()
                            seq = analyzer(sent)
                            indices = []
                            for w in seq:
                                try:
                                    indices.append(self.vocabulary_[w])
                                except KeyError:
                                    continue
                            return indices
                </code></pre>
            </div>
            <div class='post home coding' id='saveloadpickle'>
                <h1>Save and load models with <code>pickle</code></h1>
                <p>There exist plenty formats for our <strong>ML</strong> models, as different as they are, we can actually use only one tool to load and save models in 
                    whichever format: 
                <code>pickle</code>.</p>
                <pre><code class='python'>
                    def save_model(model, filename):
                        print('saving model to %s ...' % filename)
                        pickle.dump(model, open(filename, 'wb'))
                    
                    def load_model(filename):
                        print('loading model from %s ...' % filename)
                        return pickle.load(open(filename, 'rb'))    
                </code></pre>
                <p>Assume you just create and fit a <code>CountVectorizer</code> object, just simply save and load the model with:</p>
                <pre><code class='python'>
                    save_model(your_object, your_savename_here)
                    model = load_model(your_savename_here)
                </code></pre>
                <p><strong>P/S:</strong> When loading the model with pickle in a different file than when you define it, 
                    you have to also re-import the object type in this file, otherwise <code>pickle</code> will complain about unknown object type.</p>
            </div>
            <div class='post home coding' id='wordcloud'>
                <h1><i>Topic modeling: </i>Vizualize multiple wordclouds with correlation edges</h1>

                <p>In this post, you will learn how to generate multiple wordclouds and from which create a graph with correlation edges with <code>matplotlib</code> and 
                <code>wordcloud</code>.</p>
                <p>Topic modeling is a classic task in <b>NLP</b> and <b>text-mining</b>. The objective is to extract topics from raw texts and find a good way
                to interprete them. Why the interpretation? Because texts will be transformed to dense vectors, then our models will train on those vectors to produce probable topics,
                under form of vectors. But a dense vector topic says little to us, we have to convert it back to words. And this part we still struggle to solve completely. Normally each topic
                will be represented by a bunch of words apparently most coherent (computed via their frequencies amongst documents of same topic or their covariances with 
                topic vector, it's your choice your test), and nothing visualize a bunch of words better than a wordcloud.</p>
                
                <p>We already know how to use <code>wordcloud</code> package to create beautiful wordclouds in whichever shape we want. But how about puting together multiple 
                wordcloud into one window and connect those well correlated. Nothing says better than an image:</p>
                <img src='img/wordclouds_.png' width=800/>

                <p>First things firsts: Create some texts to work with:</p>

                <pre><code class='python'>
                    texts = ['Today Europe celebrates victory against Nazis Germany in World War II',
                            'Things turn out complicated for Hillary Clinton',
                            'Nothing is better than a glass of milk in the morning']
                </code></pre>

                <p>Now let's create the wordclouds:</p>

                <pre><code class='python'>
                    # first define some parameters to make the wordclouds more lisible
                    kwargs = {'background_color': 'rgba(0, 0, 0, 0)', 'mode': 'RGBA', 'colormap': 'Spectral'}
                    topic_clouds = [WordCloud(**kwargs).generate(phrase) for phrase in texts]
                </code></pre>

                <p>Next, we define to back-bone functions, get them and you can already draw graph (graph with vertices and edges) on <code>matplotlib</code>.</p>

                <pre><code class='python'>
                    def draw_line(ax, p1, p2, linewidth, color='black'):
                        ax.annotate(p2,
                                    xy=p2, xycoords='data',
                                    xytext=p1, size=20, va="center", ha="center",
                                    arrowprops=dict(arrowstyle="<|-|>",
                                                    connectionstyle="arc3,rad=-0.2",
                                                    fc="w", linewidth=linewidth, color=color))
                    
                    def im_annot(im_array, coords, zoom=0.5):
                        imagebox = OffsetImage(im_array, interpolation='bilinear', zoom=zoom)
                        annot = AnnotationBbox(imagebox, coords, 
                                            bboxprops=dict(facecolor='none', edgecolor='none', boxstyle='round'),
                                          )
                        return annot
                </code></pre>

                <p>Finally the plotting code:</p>

                <pre><code class='python'>
                        coords = np.random.randint(2, size=(3, 2)) # define the coords for wordclouds centroids
                        lim = np.max(coords, axis=0)
                        
                        fig, ax = plt.subplots(figsize=(8, 8))
                        
                        ax.set_xlim(-0.5, lim[0] + 0.5)
                        ax.set_ylim(-0.5, lim[1] + 0.5)
                        
                        for i, topic in enumerate(topic_clouds):
                            ax.add_artist(im_annot(topic.to_array(), coords[i])) # draw wordcloud on pre-assigned coords
                            for j in range(3):
                                if i >= j:
                                    continue
                                draw_line(ax, coords[i], coords[j], 1) # draw edge with all other clouds
                            
                        ax.set_yticklabels([]) #disable ticks
                        ax.set_xticklabels([]) #disable ticks
                        
                        plt.grid(None)  #disable gridlines                      
                        plt.show()
                </code></pre>
                <p>And here the result:</p>
                <img src='img/ex_wordclouds.png' width=400 />

                <p>This would be very helpful when you do some keyword or topic extraction, for example. Imagine you do topic modeling with <b>LDA</b> and you want to 
                visualize how different topics correlate, you can assign their correlations to the edge weights between them. And that's it.</p>
            </div>
        </div>
        
        
        
        <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
        
        <script src="js/jquery-3.4.1.min.js"></script>
        <!-- my js -->
        <script src='js/main.js'></script>
        <script src='js/outline.js'></script>
        <script src='js/navi.js'></script>
        <script src='js/app.js'></script>
        <script src='js/post.js'></script>
        
        <!-- code syntax highlighting-->
        <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.15.6/build/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        
    </body>
</html>